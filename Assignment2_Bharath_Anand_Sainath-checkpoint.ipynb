{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment2 \n",
    "#Title: Implementation of K Nearest Neighbors algorithm for Digit Recognition\n",
    "#Professor: Dr.Mark Albert\n",
    "#Authors: Bharath Kumar Suroju (11334019), Sainath Reddy Yaramada(11332429), Anand Sagar Pandrapagada(11197418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples is  1797\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Loading digits data set\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "print(\"Total number of samples is \",n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing data\n",
    "data, target = digits.data , digits.target\n",
    "\n",
    "#reshaping the data\n",
    "data = digits.images.reshape((n_samples,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting data into 50:50 ratio\n",
    "X_train,X_test,y_train,y_test = train_test_split(data, target, test_size = 0.5,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(X_train, X_test, k):\n",
    "    euc_dist = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Calculating the euclidean distances from each of the training data\n",
    "            a = np.sum(np.square(X_test - X_train[i,:]))\n",
    "            dist_calc = np.sqrt(a)\n",
    "            euc_dist.append([dist_calc,i])\n",
    "    #sorting to get nearest at the top\n",
    "    return sorted(euc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "#iterating the k nearest neighbors\n",
    "def target_collection(distance,y_train,k):\n",
    "    targets=[]\n",
    "    for j in range(k):\n",
    "        #index of the current distance and taking the most common ones for voting\n",
    "        ind = distance[j][1]\n",
    "        targets.append(y_train[ind])\n",
    "        y_pred = collections.Counter(targets)\n",
    "    return y_pred.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# k is number of nearest neighbors defined \n",
    "def KNN(X_train, y_train, X_test, k):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        \n",
    "        #call the distance calculation function\n",
    "        dist = euclidean_distance(X_train,X_test[i,:],k)\n",
    "        \n",
    "        #get the y\n",
    "        y = target_collection(dist,y_train,k)\n",
    "        predictions.append(y)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_list = [1,3,5,100,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy for k = 1 is 98.33147942157953%\n",
      "Confusion matrix \n",
      " [[82  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 89  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 82  0  0  0  0  0  1  0]\n",
      " [ 0  0  0 90  0  0  0  2  1  0]\n",
      " [ 0  1  0  0 92  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 97  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 98  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 86  0  1]\n",
      " [ 0  3  0  0  0  0  0  0 80  0]\n",
      " [ 0  0  0  2  1  0  0  0  1 88]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        82\n",
      "           1       0.96      1.00      0.98        89\n",
      "           2       1.00      0.99      0.99        83\n",
      "           3       0.98      0.97      0.97        93\n",
      "           4       0.99      0.99      0.99        93\n",
      "           5       1.00      0.98      0.99        99\n",
      "           6       1.00      1.00      1.00        98\n",
      "           7       0.98      0.99      0.98        87\n",
      "           8       0.96      0.96      0.96        83\n",
      "           9       0.97      0.96      0.96        92\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       899\n",
      "   macro avg       0.98      0.98      0.98       899\n",
      "weighted avg       0.98      0.98      0.98       899\n",
      "\n",
      "Accuarcy for k = 3 is 98.66518353726363%\n",
      "Confusion matrix \n",
      " [[82  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 89  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 83  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 89  0  0  0  2  2  0]\n",
      " [ 0  0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 97  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 98  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 87  0  0]\n",
      " [ 0  1  0  0  0  0  0  1 81  0]\n",
      " [ 0  0  0  1  1  1  0  0  1 88]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        82\n",
      "           1       0.99      1.00      0.99        89\n",
      "           2       1.00      1.00      1.00        83\n",
      "           3       0.99      0.96      0.97        93\n",
      "           4       0.99      1.00      0.99        93\n",
      "           5       0.99      0.98      0.98        99\n",
      "           6       1.00      1.00      1.00        98\n",
      "           7       0.97      1.00      0.98        87\n",
      "           8       0.96      0.98      0.97        83\n",
      "           9       0.98      0.96      0.97        92\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n",
      "Accuarcy for k = 5 is 97.77530589543937%\n",
      "Confusion matrix \n",
      " [[82  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 89  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 83  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 88  0  0  0  2  3  0]\n",
      " [ 0  0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 95  1  0  0  3]\n",
      " [ 1  0  0  0  0  0 97  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 87  0  0]\n",
      " [ 0  3  0  0  0  0  0  1 78  1]\n",
      " [ 0  0  0  1  1  1  0  1  1 87]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        82\n",
      "           1       0.97      1.00      0.98        89\n",
      "           2       1.00      1.00      1.00        83\n",
      "           3       0.99      0.95      0.97        93\n",
      "           4       0.99      1.00      0.99        93\n",
      "           5       0.99      0.96      0.97        99\n",
      "           6       0.99      0.99      0.99        98\n",
      "           7       0.96      1.00      0.98        87\n",
      "           8       0.95      0.94      0.95        83\n",
      "           9       0.96      0.95      0.95        92\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       899\n",
      "   macro avg       0.98      0.98      0.98       899\n",
      "weighted avg       0.98      0.98      0.98       899\n",
      "\n",
      "Accuarcy for k = 100 is 88.54282536151278%\n",
      "Confusion matrix \n",
      " [[82  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 69 10  0  0  0  2  0  2  6]\n",
      " [ 0  0 76  3  0  0  0  1  3  0]\n",
      " [ 0  1  0 80  0  0  0  6  5  1]\n",
      " [ 1  4  0  0 85  0  0  3  0  0]\n",
      " [ 2  0  0  0  1 79  2  0  0 15]\n",
      " [ 2  0  0  0  0  0 96  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 87  0  0]\n",
      " [ 0  5  1  2  0  1  0  1 69  4]\n",
      " [ 1  4  0  3  1  1  0  7  2 73]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        82\n",
      "           1       0.83      0.78      0.80        89\n",
      "           2       0.87      0.92      0.89        83\n",
      "           3       0.91      0.86      0.88        93\n",
      "           4       0.98      0.91      0.94        93\n",
      "           5       0.98      0.80      0.88        99\n",
      "           6       0.96      0.98      0.97        98\n",
      "           7       0.83      1.00      0.91        87\n",
      "           8       0.85      0.83      0.84        83\n",
      "           9       0.74      0.79      0.76        92\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       899\n",
      "   macro avg       0.89      0.89      0.88       899\n",
      "weighted avg       0.89      0.89      0.89       899\n",
      "\n",
      "Accuarcy for k = 500 is 50.723025583982206%\n",
      "Confusion matrix \n",
      " [[82  0  0  0  0  0  0  0  0  0]\n",
      " [ 3 15 12  4  0  0  2  2 47  4]\n",
      " [ 2  1 58 12  0  0  0  2  8  0]\n",
      " [10  0  0 74  0  0  0  6  3  0]\n",
      " [38  2  0  0 45  0  0  6  2  0]\n",
      " [45  0  6 13  0 24  0  5  6  0]\n",
      " [76  0  0  0  0  0 21  0  1  0]\n",
      " [ 7  0  0  0  1  0  0 74  5  0]\n",
      " [11  1  2 10  0  1  2  2 53  1]\n",
      " [59  0  0 12  3  0  0  5  3 10]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40        82\n",
      "           1       0.79      0.17      0.28        89\n",
      "           2       0.74      0.70      0.72        83\n",
      "           3       0.59      0.80      0.68        93\n",
      "           4       0.92      0.48      0.63        93\n",
      "           5       0.96      0.24      0.39        99\n",
      "           6       0.84      0.21      0.34        98\n",
      "           7       0.73      0.85      0.78        87\n",
      "           8       0.41      0.64      0.50        83\n",
      "           9       0.67      0.11      0.19        92\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       899\n",
      "   macro avg       0.69      0.52      0.49       899\n",
      "weighted avg       0.70      0.51      0.49       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "for a in K_list:\n",
    "    predictions = KNN(X_train, y_train, X_test,a)\n",
    "    correct = 0\n",
    "    #calculating the accuracy\n",
    "    for x in range(len(y_test)):\n",
    "        if y_test[x] == predictions[x]:\n",
    "            correct +=1\n",
    "            confuse = confusion_matrix(y_test,predictions) \n",
    "    accuracy = (correct/len(y_test)) * 100.0\n",
    "    print(\"Accuarcy for k = \" + str(a) + \" is \" + str(accuracy) + \"%\")\n",
    "    print(\"Confusion matrix \\n\", confusion_matrix(y_test,predictions))\n",
    "    print(\"classification report \\n\", classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
